# Rule Generation System Design

## Overview

The rule generation system transforms user instructions into abstract, reusable rules using an LLM. Instead of storing instructions verbatim, the system extracts the core principle into a concise, actionable rule.

## Core Concept

When a user provides an instruction like "You have mocked the CalculateSumService, which is an internal module. Don't do that, and instead make sure all internal code is executed. Only mock external dependencies like API calls.", the system should generate an abstract rule like "Don't mock internal modules" that can be applied across similar contexts.

## Architecture Components

### 1. Rule Generation Pipeline

```
User Instruction → LLM Processing → Rule Storage
```

#### Input

- **User Instruction**: The verbatim instruction from the developer
- **Context**: Development context (e.g., "Testing external API integrations", "React components")

#### LLM Processing

- **Prompt Engineering**: Simple prompt to extract the core rule
- **Rule Abstraction**: Transform specific instructions into abstract patterns
- **Category Assignment**: Automatically categorize rules (testing, naming, architecture, etc.)

#### Output

- **Abstract Rule**: Concise, actionable rule text
- **Category**: Rule category for organization
- **Context**: Preserved context from input

### 2. Database Schema

Following the existing schema from ROADMAP.md:

```sql
-- Instructions table: stores verbatim developer instructions
instructions (
  id INTEGER PRIMARY KEY,
  instruction TEXT NOT NULL,
  timestamp DATETIME DEFAULT CURRENT_TIMESTAMP NOT NULL,
  context TEXT NOT NULL,
  rule_id INTEGER,
  FOREIGN KEY (rule_id) REFERENCES rules(id)
)

-- Rules table: stores abstract rules derived from instructions
rules (
  id INTEGER PRIMARY KEY,
  rule_text TEXT NOT NULL, -- The abstract rule generated by LLM
  category TEXT, -- e.g., "testing", "naming", "architecture"
  context TEXT, -- specific situation, e.g., "Testing external API integrations"
  relevance_score FLOAT DEFAULT 1.0,
  embedding BLOB, -- vector embedding (for future use)
  created_from_instruction_id INTEGER,
  last_applied DATETIME,
  instructions_count INTEGER DEFAULT 0,
  FOREIGN KEY (created_from_instruction_id) REFERENCES instructions(id)
)
```

### 3. LLM Integration Design

#### Prompt Structure

```
System: You are a coding assistant that extracts abstract rules from developer instructions.

Task: Convert the following user instruction into a concise, abstract rule.

User Instruction: "{instruction}"
Context: "{context}"

Requirements:
1. Extract the core principle into a short, actionable rule
2. Make it general enough to apply to similar situations
3. Assign an appropriate category (testing, naming, architecture, documentation, error-handling, performance, security)

Output Format (JSON):
{
  "rule_text": "Concise, abstract rule",
  "category": "category_name"
}
```

#### Example Transformations

| User Instruction                                                                                                                                                                            | Abstract Rule                                | Category       |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- | -------------- |
| "You have mocked the CalculateSumService, which is an internal module. Don't do that, and instead make sure all internal code is executed. Only mock external dependencies like API calls." | "Don't mock internal modules"                | testing        |
| "Use PascalCase for component names"                                                                                                                                                        | "Use PascalCase for component names"         | naming         |
| "Always add error handling for database operations"                                                                                                                                         | "Add error handling for database operations" | error-handling |

### 4. Rule Categories

- **testing**: Unit tests, integration tests, test patterns
- **naming**: Naming conventions, variable names, function names
- **architecture**: Code structure, design patterns, organization
- **documentation**: Comments, README, API docs
- **error-handling**: Exception handling, error recovery
- **performance**: Optimization, efficiency, resource usage
- **security**: Authentication, authorization, data protection
- **style**: Code formatting, linting rules
- **best-practices**: General coding standards

### 5. Implementation Phases

#### Phase 1: Basic LLM Integration

- [x] Set up LLM client (Use the OpenAI SDK). Make sure it can be easily mocked in tests.
- [x] Create rule generation prompt templates
- [x] Implement basic rule generation function
- [x] Update existing `rememberDeveloperInstruction` tool to use LLM

#### Phase 2: Enhanced Processing

- [ ] Add category auto-assignment
- [ ] Add duplicate detection for similar rules
- [ ] Implement rule validation
- [ ] Add rule effectiveness tracking

#### Phase 3: Advanced Features

- [ ] Add rule consolidation logic (merge similar rules)
- [ ] Implement rule versioning
- [ ] Add rule conflict detection
- [ ] Create rule recommendation system

### 6. Technical Considerations

#### LLM Selection

- **OpenAI GPT-4**: High quality, reliable, but requires API calls
- **Anthropic Claude**: Good reasoning, safety-focused
- **Local Models**: Privacy-focused, but may require more setup

#### Error Handling

- **Retry Logic**: Retry failed generations to account for network issues
- **Manual Override**: Allow manual rule creation when needed

### 7. Integration with Existing System

#### Updated `rememberDeveloperInstruction` Tool

The existing tool will be enhanced to:

1. Accept user instruction and context
2. Call LLM to generate abstract rule
3. Store both original instruction and generated rule
4. Return rule information to user

#### Flow

```
User Instruction → LLM Processing → Rule Generation → Database Storage → Response
```

#### Example Usage

```typescript
// Current tool call
await rememberDeveloperInstruction({
  instruction: "You have mocked the CalculateSumService, which is an internal module. Don't do that.",
  context: "Testing external API integrations"
});

// Expected LLM output
{
  "rule_text": "Don't mock internal modules",
  "category": "testing"
}
```

### 8. Success Metrics

- **Rule Quality**: Developer satisfaction with generated rules
- **Rule Reusability**: How often rules are applied across different contexts
- **Generation Accuracy**: Success rate of rule generation
- **Developer Productivity**: Reduction in repeated instructions

### 9. Future Enhancements

- **Rule Templates**: Pre-defined rule patterns for common scenarios
- **Collaborative Rules**: Team-based rule sharing and voting
- **Rule Evolution**: Automatic rule improvement based on usage patterns
- **Integration APIs**: Connect with other development tools and IDEs
